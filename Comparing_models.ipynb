{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparing models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0554ba8ee9a41a680cacacc4c47b7b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaf41becaa924be9baa7be6e72057e44",
              "IPY_MODEL_d4fb7e0ac4b04eee97d6ab8da4bb0ab2",
              "IPY_MODEL_ecbfe4d9c3d34e99ab54d0fdfefde497"
            ],
            "layout": "IPY_MODEL_c172a6fb6cb74768ad445659ee2c2775"
          }
        },
        "aaf41becaa924be9baa7be6e72057e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f331f5bdb3f45dbb2fc9dde9b050a3d",
            "placeholder": "​",
            "style": "IPY_MODEL_24f7e6e00c164296b3550f668489a339",
            "value": "differential training: 100%"
          }
        },
        "d4fb7e0ac4b04eee97d6ab8da4bb0ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fce12c29b71e4062ae798f4dfe55e6f5",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_105991a897f3452ba99b93c2db0995b3",
            "value": 100
          }
        },
        "ecbfe4d9c3d34e99ab54d0fdfefde497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a69ef5584b4a308ba8ec3ea67adf6a",
            "placeholder": "​",
            "style": "IPY_MODEL_026a89481a074cfb8b95e0584d899e15",
            "value": " 100/100 [00:02&lt;00:00, 39.55it/s]"
          }
        },
        "c172a6fb6cb74768ad445659ee2c2775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f331f5bdb3f45dbb2fc9dde9b050a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f7e6e00c164296b3550f668489a339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fce12c29b71e4062ae798f4dfe55e6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105991a897f3452ba99b93c2db0995b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15a69ef5584b4a308ba8ec3ea67adf6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "026a89481a074cfb8b95e0584d899e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkIFriemdan/hse-ftiad-thesis-bayesian-neural-networks-for-derivatives-valuation-approximation/blob/main/Comparing_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-vU1GoLUK5L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix seeds for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLODv9gSX12A",
        "outputId": "5a735df1-c25c-4454-c72b-8c1e8ca8cba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f57e68a4eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate data\n",
        " **bsPrice** we calculate using the following formula \n",
        "\n",
        "\n",
        "${\\displaystyle {\\begin{aligned}P(S_{t},t)&=Ke^{-r(T-t)}-S_{t}+C(S_{t},t)\\\\&=N(-d_{2})Ke^{-r(T-t)}-N(-d_{1})S_{t}\\end{aligned}}\\,}$"
      ],
      "metadata": {
        "id": "DBrX7ofpVnqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros((1, 5))\n",
        "b = a.copy()\n",
        "np.vstack([a, b]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUddVREm9uLL",
        "outputId": "2e44c560-a509-4624-85e3-f240c3e297bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ATM call Vanilla options"
      ],
      "metadata": {
        "id": "B8a8NMDjRqI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper analytics    \n",
        "\n",
        "def ds(K, S, T, vol, r, q=0):\n",
        "    vol_t = vol * np.sqrt(T)\n",
        "    d1 = (np.log(S / K) + (r - q + 0.5 * vol * vol) * T) / vol_t\n",
        "    d2 = d1 - vol_t\n",
        "    return d1, d2\n",
        "\n",
        "def call(K, S, T, vol, r, q=0):\n",
        "    disc = np.exp(-r * T)\n",
        "    pv_k = K * disc\n",
        "    spot_after_div = S * np.exp(-q * T)\n",
        "\n",
        "    d1, d2 = ds(K, S, T, vol, r, q)\n",
        "    c = norm.cdf(d1) * spot_after_div - norm.cdf(d2) * pv_k\n",
        "    return c\n",
        "\n",
        "def greeks(K, S, T, vol, r, q=0):\n",
        "    disc = np.exp(-r * T)\n",
        "    pv_k = K * disc\n",
        "    d1, d2 = ds(K, S, T, vol, r, q)\n",
        "    delta = norm.cdf(d1)\n",
        "    theta = - S * norm.pdf(d1) * vol / (2 * np.sqrt(T)) - r * pv_k * norm.cdf(d2)\n",
        "    vega = S * norm.pdf(d1) * np.sqrt(T)\n",
        "    rho = pv_k * norm.cdf(d2) * T\n",
        "\n",
        "    return delta, theta, vega, rho\n",
        "\n",
        "v_call = np.vectorize(call)\n",
        "v_greeks = np.vectorize(greeks)\n",
        "\n",
        "# main class\n",
        "# в качестве параметров выбираем S, T, sigma (vol), r\n",
        "class DataGen:\n",
        "    def __init__(self, S_range, T_range, sigma_range, r_range):\n",
        "        self.S_range = S_range\n",
        "        self.T_range = T_range \n",
        "        self.sigma_range = sigma_range\n",
        "        self.r_range = r_range\n",
        "\n",
        "    def dataset(self, n_samples, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        S = np.random.uniform(*self.S_range, n_samples)\n",
        "        K = S.copy() # at the money option (ATM) \n",
        "        T = np.random.uniform(*self.T_range, n_samples)\n",
        "        vol = np.random.uniform(*self.sigma_range, n_samples)\n",
        "        r = np.random.uniform(*self.r_range, n_samples)\n",
        "\n",
        "        X = np.vstack([S, T, vol, r]).T\n",
        "        y = v_call(K, S, T, vol, r).reshape((-1, 1))\n",
        "        dy = np.vstack(v_greeks(K, S, T, vol, r)).T\n",
        "        return X, y, dy\n"
      ],
      "metadata": {
        "id": "BB5Pi2olVpXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate train and test"
      ],
      "metadata": {
        "id": "PMRo4xT-h_L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 10000\n",
        "\n",
        "rng = {\n",
        "    \"spot\": (0.5, 2),\n",
        "    \"time\": (0, 3.0),\n",
        "    \"sigma\": (0.1, 0.5),\n",
        "    \"rate\": (-0.01, 0.03)\n",
        "}"
      ],
      "metadata": {
        "id": "nrpmh1Xrhmpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generator = DataGen(rng['spot'], rng['time'], rng['sigma'], rng['rate'])\n",
        "xTrain, yTrain, dydxTrain = generator.dataset(n_samples, seed=seed)\n",
        "xTest, yTest, dydxTest = generator.dataset(n_samples // 5, seed=100)\n",
        "xAxis = xTest.copy()"
      ],
      "metadata": {
        "id": "qtwlSA3FcWQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0edb0b-6e19-4acd-9edb-ae1069124b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.3 s, sys: 383 ms, total: 13.7 s\n",
            "Wall time: 18 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RiskFuel"
      ],
      "metadata": {
        "id": "qmslwGvcHDot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_layers, n_output):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.linears = torch.nn.ModuleList([torch.nn.Linear(n_feature, n_hidden)])\n",
        "        self.linears.extend([torch.nn.Linear(n_hidden, n_hidden) for i in range(1, n_layers)])\n",
        "        self.linears.append(torch.nn.Linear(n_hidden, n_output))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for lin in self.linears:\n",
        "            x = F.relu(lin(x))       # Activation function for all layers (prices can't be negative)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vp9uliw-95SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(xTrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW3LPrbWbE06",
        "outputId": "33b05128-335b-4942-9ca3-e63f15440f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "\n",
        "to_tensor = lambda x: torch.from_numpy(x).float().to(device)\n",
        "\n",
        "def fit_net(net: Net, n_epochs: int, x_train: np.ndarray, y_train: np.ndarray,\n",
        "            x_test: np.ndarray, y_test: np.ndarray, device: str='cpu'):\n",
        "\n",
        "    n_train = x_train.shape[0]\n",
        "    n_test = x_test.shape[0]\n",
        "    net.to(device)\n",
        "\n",
        "    x_ = to_tensor(x_train)\n",
        "    y_ = to_tensor(y_train)\n",
        "\n",
        "    x_test_ = to_tensor(x_test)\n",
        "    y_test_ = to_tensor(y_test)\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "    loss_func = torch.nn.MSELoss()\n",
        "   \n",
        "    l = 10**5\n",
        "    best_l = 1e-3\n",
        "    checkpoint = {}\n",
        "    l_train = []\n",
        "    l_test = []\n",
        "\n",
        "    \n",
        "    for e in range(n_epochs):\n",
        "        prediction = net(x_)\n",
        "        loss = loss_func(prediction, y_)\n",
        "        l_train.append(loss.data.cpu().numpy())\n",
        "\n",
        "        prediction_test = net(x_test_)\n",
        "        loss_test = loss_func(prediction_test, y_test_)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        l = loss_test.data.cpu().numpy()\n",
        "        l_test.append(l)\n",
        "        if l.item() < best_l:\n",
        "            best_l = l.item()\n",
        "            checkpoint = {\n",
        "                \"n_hidden\": net.n_hidden,\n",
        "                \"n_layers\": net.n_layers,\n",
        "                \"model_state_dict\": net.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            }\n",
        "        if (e + 1) % 100 == 0:\n",
        "            print(f\"\\tEpoch: {e+1}\\tL2 Loss = {loss.data.cpu().numpy()}\")\n",
        "\n",
        "    return best_l, checkpoint, l_train, l_test"
      ],
      "metadata": {
        "id": "35C7xi-xZyms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    print(f\"GPU detected. Running on {device}\")\n",
        "else:\n",
        "    print(\"No GPU detected. Running on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55KmiP9_Zzih",
        "outputId": "5d33ee33-a8d4-48f5-e81e-c03153dddb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU detected. Running on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(n_feature=4, n_hidden=100, n_layers=4, n_output=1)  # define the network\n",
        "print(net)  # net architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zoqMqhUaC6b",
        "outputId": "7d849b21-aef0-41a0-9003-7e907afe4e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (linears): ModuleList(\n",
            "    (0): Linear(in_features=4, out_features=100, bias=True)\n",
            "    (1): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (4): Linear(in_features=100, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "n_epochs = 100\n",
        "ls, checkpoint, l_train, l_test = fit_net(net, n_epochs, xTrain, yTrain, xTest, yTest, device)\n",
        "print(f\"Best loss ={ls}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rDB9UECaEwf",
        "outputId": "2cc61073-2c48-4bbb-b083-4907b6d15e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 100\tL2 Loss = 0.00015619499026797712\n",
            "Best loss =0.00016891414998099208\n",
            "CPU times: user 1.82 s, sys: 978 ms, total: 2.8 s\n",
            "Wall time: 2.79 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint"
      ],
      "metadata": {
        "id": "23uU1-oJSuZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(n_feature=4,\n",
        "            n_hidden=checkpoint[\"n_hidden\"],\n",
        "            n_layers=checkpoint[\"n_layers\"],\n",
        "            n_output=1)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zv5VFkoRPPy",
        "outputId": "a3e688a1-7293-4ca0-ab11-98d623c2dc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (linears): ModuleList(\n",
              "    (0): Linear(in_features=4, out_features=100, bias=True)\n",
              "    (1): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (4): Linear(in_features=100, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "y_pred = model(to_tensor(xTest).to(device)).flatten().data.cpu().numpy()\n",
        "mse_err = mse(yTest, y_pred)\n",
        "print(\"mse error on test set for RiskFuel is %.5f\" %mse_err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_tq8SuWRYwC",
        "outputId": "80cb975f-5030-4064-f7f6-ef11dd9fb6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mse error on test set for RiskFuel is 0.00018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DifferentialML"
      ],
      "metadata": {
        "id": "cz5JekdUHH1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    %matplotlib notebook\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# import and test\n",
        "import tensorflow as tf2\n",
        "print(\"TF version =\", tf2.__version__)\n",
        "\n",
        "# we want TF 2.x\n",
        "assert tf2.__version__ >= \"2.0\"\n",
        "\n",
        "# disable eager execution etc\n",
        "tf = tf2.compat.v1\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "# disable annoying warnings\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# make sure we have GPU support\n",
        "print(\"GPU support = \", tf.test.is_gpu_available())\n",
        "\n",
        "# import other useful libs\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "# representation of real numbers in TF, change here for 32/64 bits\n",
        "real_type = tf.float32\n",
        "# real_type = tf.float64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtIKoWeIKD2J",
        "outputId": "a58f81cf-31f2-4adb-8495-33e31be88faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version = 2.8.2\n",
            "GPU support =  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_net(\n",
        "    input_dim,      # dimension of inputs, e.g. 10\n",
        "    hidden_units,   # units in hidden layers, assumed constant, e.g. 20\n",
        "    hidden_layers,  # number of hidden layers, e.g. 4\n",
        "    seed):          # seed for initialization or None for random\n",
        "    \n",
        "    # set seed\n",
        "    tf.set_random_seed(seed)\n",
        "    \n",
        "    # input layer\n",
        "    xs = tf.placeholder(shape=[None, input_dim], dtype=real_type)\n",
        "    \n",
        "    # connection weights and biases of hidden layers\n",
        "    ws = [None]\n",
        "    bs = [None]\n",
        "    # layer 0 (input) has no parameters\n",
        "    \n",
        "    # layer 0 = input layer\n",
        "    zs = [xs] # eq.3, l=0\n",
        "    \n",
        "    # first hidden layer (index 1)\n",
        "    # weight matrix\n",
        "    ws.append(tf.get_variable(\"w1\", [input_dim, hidden_units], \\\n",
        "        initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "    # bias vector\n",
        "    bs.append(tf.get_variable(\"b1\", [hidden_units], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # graph\n",
        "    zs.append(zs[0] @ ws[1] + bs[1]) # eq. 3, l=1\n",
        "    \n",
        "    # second hidden layer (index 2) to last (index hidden_layers)\n",
        "    for l in range(1, hidden_layers): \n",
        "        ws.append(tf.get_variable(\"w%d\"%(l+1), [hidden_units, hidden_units], \\\n",
        "            initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "        bs.append(tf.get_variable(\"b%d\"%(l+1), [hidden_units], \\\n",
        "            initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "        zs.append(tf.nn.softplus(zs[l]) @ ws[l+1] + bs[l+1]) # eq. 3, l=2..L-1\n",
        "\n",
        "    # output layer (index hidden_layers+1)\n",
        "    ws.append(tf.get_variable(\"w\"+str(hidden_layers+1), [hidden_units, 1], \\\n",
        "            initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "    bs.append(tf.get_variable(\"b\"+str(hidden_layers+1), [1], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # eq. 3, l=L\n",
        "    zs.append(tf.nn.softplus(zs[hidden_layers]) @ ws[hidden_layers+1] + bs[hidden_layers+1]) \n",
        "    \n",
        "    # result = output layer\n",
        "    ys = zs[hidden_layers+1]\n",
        "    \n",
        "    # return input layer, (parameters = weight matrices and bias vectors), \n",
        "    # [all layers] and output layer\n",
        "    return xs, (ws, bs), zs, ys\n",
        "\n",
        "\n",
        "    # compute d_output/d_inputs by (explicit) backprop in vanilla net\n",
        "def backprop(\n",
        "    weights_and_biases, # 2nd output from vanilla_net() \n",
        "    zs):                # 3rd output from vanilla_net()\n",
        "    \n",
        "    ws, bs = weights_and_biases\n",
        "    L = len(zs) - 1\n",
        "    \n",
        "    # backpropagation, eq. 4, l=L..1\n",
        "    zbar = tf.ones_like(zs[L]) # zbar_L = 1\n",
        "    for l in range(L-1, 0, -1):\n",
        "        zbar = (zbar @ tf.transpose(ws[l+1])) * tf.nn.sigmoid(zs[l]) # eq. 4\n",
        "    # for l=0\n",
        "    zbar = zbar @ tf.transpose(ws[1]) # eq. 4\n",
        "    \n",
        "    xbar = zbar # xbar = zbar_0\n",
        "    \n",
        "    # dz[L] / dx\n",
        "    return xbar    \n",
        "\n",
        "# combined graph for valuation and differentiation\n",
        "def twin_net(input_dim, hidden_units, hidden_layers, seed):\n",
        "    \n",
        "    # first, build the feedforward net\n",
        "    xs, (ws, bs), zs, ys = vanilla_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # then, build its differentiation by backprop\n",
        "    xbar = backprop((ws, bs), zs)\n",
        "    \n",
        "    # return input x, output y and differentials d_y/d_z\n",
        "    return xs, ys, xbar"
      ],
      "metadata": {
        "id": "gGz9Pwc1HKHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_training_graph(input_dim, hidden_units, hidden_layers, seed):\n",
        "    \n",
        "    # net\n",
        "    inputs, weights_and_biases, layers, predictions = \\\n",
        "        vanilla_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # backprop even though we are not USING differentials for training\n",
        "    # we still need them to predict derivatives dy_dx \n",
        "    derivs_predictions = backprop(weights_and_biases, layers)\n",
        "    \n",
        "    # placeholder for labels\n",
        "    labels = tf.placeholder(shape=[None, 1], dtype=real_type)\n",
        "    \n",
        "    # loss \n",
        "    loss = tf.losses.mean_squared_error(labels, predictions)\n",
        "    \n",
        "    # optimizer\n",
        "    learning_rate = tf.placeholder(real_type)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    \n",
        "    # return all necessary \n",
        "    return inputs, labels, predictions, derivs_predictions, learning_rate, loss, optimizer.minimize(loss)\n",
        "\n",
        "def vanilla_train_one_epoch(# training graph from vanilla_training_graph()\n",
        "                            inputs, labels, lr_placeholder, minimizer,   \n",
        "                            # training set \n",
        "                            x_train, y_train,                           \n",
        "                            # params, left to client code\n",
        "                            learning_rate, batch_size, session):        \n",
        "    \n",
        "    m, n = x_train.shape\n",
        "    \n",
        "    # minimization loop over mini-batches\n",
        "    first = 0\n",
        "    last = min(batch_size, m)\n",
        "    while first < m:\n",
        "        session.run(minimizer, feed_dict = {\n",
        "            inputs: x_train[first:last], \n",
        "            labels: y_train[first:last],\n",
        "            lr_placeholder: learning_rate\n",
        "        })\n",
        "        first = last\n",
        "        last = min(first + batch_size, m)\n",
        "\n",
        "def diff_training_graph(\n",
        "    # same as vanilla\n",
        "    input_dim, \n",
        "    hidden_units, \n",
        "    hidden_layers, \n",
        "    seed, \n",
        "    # balance relative weight of values and differentials \n",
        "    # loss = alpha * MSE(values) + beta * MSE(greeks, lambda_j) \n",
        "    # see online appendix\n",
        "    alpha, \n",
        "    beta,\n",
        "    lambda_j):\n",
        "    \n",
        "    # net, now a twin\n",
        "    inputs, predictions, derivs_predictions = twin_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # placeholder for labels, now also derivs labels\n",
        "    labels = tf.placeholder(shape=[None, 1], dtype=real_type)\n",
        "    derivs_labels = tf.placeholder(shape=[None, derivs_predictions.shape[1]], dtype=real_type)\n",
        "    \n",
        "    # loss, now combined values + derivatives\n",
        "    loss = alpha * tf.losses.mean_squared_error(labels, predictions) \\\n",
        "    + beta * tf. losses.mean_squared_error(derivs_labels * lambda_j, derivs_predictions * lambda_j)\n",
        "    \n",
        "    # optimizer, as vanilla\n",
        "    learning_rate = tf.placeholder(real_type)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    \n",
        "    # return all necessary tensors, including derivatives\n",
        "    # predictions and labels\n",
        "    return inputs, labels, derivs_labels, predictions, derivs_predictions, \\\n",
        "            learning_rate, loss, optimizer.minimize(loss)\n",
        "\n",
        "def diff_train_one_epoch(inputs, labels, derivs_labels, \n",
        "                         # graph\n",
        "                         lr_placeholder, minimizer,             \n",
        "                         # training set, extended\n",
        "                         x_train, y_train, dydx_train,          \n",
        "                         # params\n",
        "                         learning_rate, batch_size, session):   \n",
        "    \n",
        "    m, n = x_train.shape\n",
        "    \n",
        "    # minimization loop, now with Greeks\n",
        "    first = 0\n",
        "    last = min(batch_size, m)\n",
        "    while first < m:\n",
        "        session.run(minimizer, feed_dict = {\n",
        "            inputs: x_train[first:last], \n",
        "            labels: y_train[first:last],\n",
        "            derivs_labels: dydx_train[first:last],\n",
        "            lr_placeholder: learning_rate\n",
        "        })\n",
        "        first = last\n",
        "        last = min(first + batch_size, m)\n"
      ],
      "metadata": {
        "id": "iFrPm4tg3iES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic data preparation\n",
        "epsilon = 1.0e-08\n",
        "def normalize_data(x_raw, y_raw, dydx_raw=None, crop=None):\n",
        "    \n",
        "    # crop dataset\n",
        "    m = crop if crop is not None else x_raw.shape[0]\n",
        "    x_cropped = x_raw[:m]\n",
        "    y_cropped = y_raw[:m]\n",
        "    dycropped_dxcropped = dydx_raw[:m] if dydx_raw is not None else None\n",
        "    \n",
        "    # normalize dataset\n",
        "    x_mean = x_cropped.mean(axis=0)\n",
        "    x_std = x_cropped.std(axis=0) + epsilon\n",
        "    x = (x_cropped- x_mean) / x_std\n",
        "    y_mean = y_cropped.mean(axis=0)\n",
        "    y_std = y_cropped.std(axis=0) + epsilon\n",
        "    y = (y_cropped-y_mean) / y_std\n",
        "    \n",
        "    # normalize derivatives too\n",
        "    if dycropped_dxcropped is not None:\n",
        "        dy_dx = dycropped_dxcropped / y_std * x_std \n",
        "        # weights of derivatives in cost function = (quad) mean size\n",
        "        lambda_j = 1.0 / np.sqrt((dy_dx ** 2).mean(axis=0)).reshape(1, -1)\n",
        "    else:\n",
        "        dy_dx = None\n",
        "        lambda_j = None\n",
        "    \n",
        "    return x_mean, x_std, x, y_mean, y_std, y, dy_dx, lambda_j"
      ],
      "metadata": {
        "id": "5JoT65QL4vkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(description,\n",
        "          # neural approximator\n",
        "          approximator,              \n",
        "          # training params\n",
        "          reinit=True, \n",
        "          epochs=100, \n",
        "          # one-cycle learning rate schedule\n",
        "          learning_rate_schedule=[    (0.0, 1.0e-8), \\\n",
        "                                      (0.2, 0.1),    \\\n",
        "                                      (0.6, 0.01),   \\\n",
        "                                      (0.9, 1.0e-6), \\\n",
        "                                      (1.0, 1.0e-8)  ], \n",
        "          batches_per_epoch=16,\n",
        "          min_batch_size=256,\n",
        "          # callback function and when to call it\n",
        "          callback=None,           # arbitrary callable\n",
        "          callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "    # batching\n",
        "    batch_size = max(min_batch_size, approximator.m // batches_per_epoch)\n",
        "    \n",
        "    # one-cycle learning rate sechedule\n",
        "    lr_schedule_epochs, lr_schedule_rates = zip(*learning_rate_schedule)\n",
        "            \n",
        "    # reset\n",
        "    if reinit:\n",
        "        approximator.session.run(approximator.initializer)\n",
        "    \n",
        "    # callback on epoch 0, if requested\n",
        "    if callback and 0 in callback_epochs:\n",
        "        callback(approximator, 0)\n",
        "        \n",
        "    # loop on epochs, with progress bar (tqdm)\n",
        "    for epoch in tqdm_notebook(range(epochs), desc=description):\n",
        "        \n",
        "        # interpolate learning rate in cycle\n",
        "        learning_rate = np.interp(epoch / epochs, lr_schedule_epochs, lr_schedule_rates)\n",
        "        \n",
        "        # train one epoch\n",
        "        \n",
        "        if not approximator.differential:\n",
        "        \n",
        "            vanilla_train_one_epoch(\n",
        "                approximator.inputs, \n",
        "                approximator.labels, \n",
        "                approximator.learning_rate, \n",
        "                approximator.minimizer, \n",
        "                approximator.x, \n",
        "                approximator.y, \n",
        "                learning_rate, \n",
        "                batch_size, \n",
        "                approximator.session)\n",
        "        \n",
        "        else:\n",
        "        \n",
        "            diff_train_one_epoch(\n",
        "                approximator.inputs, \n",
        "                approximator.labels, \n",
        "                approximator.derivs_labels,\n",
        "                approximator.learning_rate, \n",
        "                approximator.minimizer, \n",
        "                approximator.x, \n",
        "                approximator.y, \n",
        "                approximator.dy_dx,\n",
        "                learning_rate, \n",
        "                batch_size, \n",
        "                approximator.session)\n",
        "        \n",
        "        # callback, if requested\n",
        "        if callback and epoch in callback_epochs:\n",
        "            callback(approximator, epoch)\n",
        "\n",
        "    # final callback, if requested\n",
        "    if callback and epochs in callback_epochs:\n",
        "        callback(approximator, epochs)    "
      ],
      "metadata": {
        "id": "lnlAUohCJ-NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_Approximator():\n",
        "    \n",
        "    def __init__(self, x_raw, y_raw, \n",
        "                 dydx_raw=None):      # derivatives labels, \n",
        "       \n",
        "        self.x_raw = x_raw\n",
        "        self.y_raw = y_raw\n",
        "        self.dydx_raw = dydx_raw\n",
        "        \n",
        "        # tensorflow logic\n",
        "        self.graph = None\n",
        "        self.session = None\n",
        "                        \n",
        "    def __del__(self):\n",
        "        if self.session is not None:\n",
        "            self.session.close()\n",
        "        \n",
        "    def build_graph(self,\n",
        "                differential,       # differential or not           \n",
        "                lam,                # balance cost between values and derivs  \n",
        "                hidden_units, \n",
        "                hidden_layers, \n",
        "                weight_seed):\n",
        "        \n",
        "        # first, deal with tensorflow logic\n",
        "        if self.session is not None:\n",
        "            self.session.close()\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "        \n",
        "        with self.graph.as_default():\n",
        "        \n",
        "            # build the graph, either vanilla or differential\n",
        "            self.differential = differential\n",
        "            \n",
        "            if not differential:\n",
        "            # vanilla \n",
        "                \n",
        "                self.inputs, \\\n",
        "                self.labels, \\\n",
        "                self.predictions, \\\n",
        "                self.derivs_predictions, \\\n",
        "                self.learning_rate, \\\n",
        "                self.loss, \\\n",
        "                self.minimizer \\\n",
        "                = vanilla_training_graph(self.n, hidden_units, hidden_layers, weight_seed)\n",
        "                    \n",
        "            else:\n",
        "            # differential\n",
        "            \n",
        "                if self.dy_dx is None:\n",
        "                    raise Exception(\"No differential labels for differential training graph\")\n",
        "            \n",
        "                self.alpha = 1.0 / (1.0 + lam * self.n)\n",
        "                self.beta = 1.0 - self.alpha\n",
        "                \n",
        "                self.inputs, \\\n",
        "                self.labels, \\\n",
        "                self.derivs_labels, \\\n",
        "                self.predictions, \\\n",
        "                self.derivs_predictions, \\\n",
        "                self.learning_rate, \\\n",
        "                self.loss, \\\n",
        "                self.minimizer = diff_training_graph(self.n, hidden_units, \\\n",
        "                                                     hidden_layers, weight_seed, \\\n",
        "                                                     self.alpha, self.beta, self.lambda_j)\n",
        "        \n",
        "            # global initializer\n",
        "            self.initializer = tf.global_variables_initializer()\n",
        "            \n",
        "        # done\n",
        "        self.graph.finalize()\n",
        "        self.session = tf.Session(graph=self.graph)\n",
        "                        \n",
        "    # prepare for training with m examples, standard or differential\n",
        "    def prepare(self, \n",
        "                m, \n",
        "                differential,\n",
        "                lam=1,              # balance cost between values and derivs  \n",
        "                # standard architecture\n",
        "                hidden_units=20, \n",
        "                hidden_layers=4, \n",
        "                weight_seed=None):\n",
        "\n",
        "        # prepare dataset\n",
        "        self.x_mean, self.x_std, self.x, self.y_mean, self.y_std, self.y, self.dy_dx, self.lambda_j = \\\n",
        "            normalize_data(self.x_raw, self.y_raw, self.dydx_raw, m)\n",
        "        \n",
        "        # build graph        \n",
        "        self.m, self.n = self.x.shape        \n",
        "        self.build_graph(differential, lam, hidden_units, hidden_layers, weight_seed)\n",
        "        \n",
        "    def train(self,            \n",
        "              description=\"training\",\n",
        "              # training params\n",
        "              reinit=True, \n",
        "              epochs=100, \n",
        "              # one-cycle learning rate schedule\n",
        "              learning_rate_schedule=[\n",
        "                  (0.0, 1.0e-8), \n",
        "                  (0.2, 0.1), \n",
        "                  (0.6, 0.01), \n",
        "                  (0.9, 1.0e-6), \n",
        "                  (1.0, 1.0e-8)], \n",
        "              batches_per_epoch=16,\n",
        "              min_batch_size=256,\n",
        "              # callback and when to call it\n",
        "              # we don't use callbacks, but this is very useful, e.g. for debugging\n",
        "              callback=None,           # arbitrary callable\n",
        "              callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "        train(description, \n",
        "              self, \n",
        "              reinit, \n",
        "              epochs, \n",
        "              learning_rate_schedule, \n",
        "              batches_per_epoch, \n",
        "              min_batch_size,\n",
        "              callback, \n",
        "              callback_epochs)\n",
        "     \n",
        "    def predict_values(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std \n",
        "        # predict scaled\n",
        "        y_scaled = self.session.run(self.predictions, feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        return y\n",
        "\n",
        "    def predict_values_and_derivs(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std\n",
        "        # predict scaled\n",
        "        y_scaled, dyscaled_dxscaled = self.session.run(\n",
        "            [self.predictions, self.derivs_predictions], \n",
        "            feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        dydx = self.y_std / self.x_std * dyscaled_dxscaled\n",
        "        return y, dydx"
      ],
      "metadata": {
        "id": "ps0Y5bEz3WVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(generator, \n",
        "         size, \n",
        "         nTest, \n",
        "         simulSeed=None, \n",
        "         testSeed=None, \n",
        "         weightSeed=None, \n",
        "         deltidx=0):\n",
        "\n",
        "    # simulation\n",
        "    print(\"simulating training, valid and test sets\")\n",
        "    xTrain, yTrain, dydxTrain = generator.dataset(size, seed=simulSeed)\n",
        "    xTest, yTest, dydxTest = generator.dataset(nTest, seed=testSeed)\n",
        "    xAxis = xTest.copy()\n",
        "    print(\"done\")\n",
        "\n",
        "    # neural approximator\n",
        "    print(\"initializing neural appropximator\")\n",
        "    regressor = Neural_Approximator(xTrain, yTrain, dydxTrain)\n",
        "    print(\"done\")\n",
        "    \n",
        "    regressor.prepare(size, False, weight_seed=weightSeed)\n",
        "    regressor.train(\"differential training\")\n",
        "    predvalues, preddiff = regressor.predict_values_and_derivs(xTest)\n",
        "\n",
        "        \n",
        "    return xAxis, yTest, dydxTest, predvalues, preddiff\n",
        "\n",
        "def graph(title, \n",
        "          predictions, \n",
        "          xAxis, \n",
        "          xAxisName, \n",
        "          yAxisName, \n",
        "          targets, \n",
        "          sizes, \n",
        "          computeRmse=False, \n",
        "          weights=None,\n",
        "          featureAxis=0):\n",
        "    \n",
        "    numRows = len(sizes)\n",
        "    numCols = 2\n",
        "\n",
        "    fig, ax = plt.subplots(numRows, numCols, squeeze=False)\n",
        "    fig.set_size_inches(4 * numCols + 1.5, 4 * numRows)\n",
        "\n",
        "    for i, size in enumerate(sizes):\n",
        "        ax[i,0].annotate(\"size %d\" % size, xy=(0, 0.5), \n",
        "          xytext=(-ax[i,0].yaxis.labelpad-5, 0),\n",
        "          xycoords=ax[i,0].yaxis.label, textcoords='offset points',\n",
        "          ha='right', va='center')\n",
        "  \n",
        "    ax[0,0].set_title(\"standard\")\n",
        "    ax[0,1].set_title(\"differential\")\n",
        "    \n",
        "    for i, size in enumerate(sizes):        \n",
        "        for j, regType, in enumerate([\"standard\", \"differential\"]):\n",
        "            targets = targets[:, 0]\n",
        "            if computeRmse:\n",
        "                errors = 100 * (predictions[(regType, size)] - targets)\n",
        "                if weights is not None:\n",
        "                    errors /= weights\n",
        "                rmse = np.sqrt((errors ** 2).mean(axis=0))\n",
        "                t = \"rmse %.2f\" % rmse\n",
        "            else:\n",
        "                t = xAxisName\n",
        "                \n",
        "            ax[i,j].set_xlabel(t)            \n",
        "            ax[i,j].set_ylabel(yAxisName)\n",
        "\n",
        "            ax[i,j].plot(xAxis*100, predictions[(regType, size)]*100, 'co', \\\n",
        "                         markersize=2, markerfacecolor='white', label=\"predicted\")\n",
        "            ax[i,j].plot(xAxis*100, targets*100, 'r.', markersize=0.5, label='targets')\n",
        "\n",
        "            ax[i,j].legend(prop={'size': 8}, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.suptitle(\"% s -- %s\" % (title, yAxisName), fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wXWoFdOi4msg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# simulation set sizes to perform\n",
        "n_samples = 10000\n",
        "\n",
        "seed = 42\n",
        "test_seed = 100\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "\n",
        "# seed\n",
        "# simulSeed = 1234\n",
        "# simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % seed)\n",
        "weightSeed = None\n",
        "\n",
        "# number of test scenarios\n",
        "nTest = n_samples // 5   \n",
        "\n",
        "# go\n",
        "generator = DataGen(rng['spot'], rng['time'], rng['sigma'], rng['rate'])\n",
        "xAxis, yTest, dydxTest, predvalues, preddiffs = test(generator, n_samples, nTest, seed, test_seed, weightSeed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "b0554ba8ee9a41a680cacacc4c47b7b9",
            "aaf41becaa924be9baa7be6e72057e44",
            "d4fb7e0ac4b04eee97d6ab8da4bb0ab2",
            "ecbfe4d9c3d34e99ab54d0fdfefde497",
            "c172a6fb6cb74768ad445659ee2c2775",
            "2f331f5bdb3f45dbb2fc9dde9b050a3d",
            "24f7e6e00c164296b3550f668489a339",
            "fce12c29b71e4062ae798f4dfe55e6f5",
            "105991a897f3452ba99b93c2db0995b3",
            "15a69ef5584b4a308ba8ec3ea67adf6a",
            "026a89481a074cfb8b95e0584d899e15"
          ]
        },
        "id": "BCqSJoxjfuM5",
        "outputId": "f37cf4c5-d8c0-4fb3-d7e3-a645d706b187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using seed 42\n",
            "simulating training, valid and test sets\n",
            "done\n",
            "initializing neural appropximator\n",
            "done\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "differential training:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0554ba8ee9a41a680cacacc4c47b7b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.2 s, sys: 350 ms, total: 12.6 s\n",
            "Wall time: 14.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_err = mse(yTest, predvalues)\n",
        "print(\"mse error on test set for RiskFuel is %.9f\" %mse_err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcTXu1FIITov",
        "outputId": "4e6ab85d-39f9-407c-cab8-2bfdd06f6758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mse error on test set for RiskFuel is 0.000003785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BNLIZ8rVN5L2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}